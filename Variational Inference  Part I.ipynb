{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <b> Variational Bayesian Methods- part I </b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this file is to intuitively explain the idea of variational inference as an approximating method of posterior distributions, and to review the key works contributing to the field of variational bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Bayesian Methods are usded in complex statistical models consisting of observed data and unknown parameters or latant variable. Unlike sampling-based inference methods, they provide an analytical approximation of the posterior distribution containing intractable integrals. \n",
    "\n",
    "### What is the idea of he variational distribution? \n",
    "\n",
    "Given some observed variables (data), the posterior distribution over a set of unobserved $W={W_1, W_2.....W_n}$ can be approximated by a known variational distribution with parameters $\\theta$\n",
    "\n",
    "$$q_\\theta(w)\\approx p(W|data)$$\n",
    "\n",
    "$q_\\theta(W)$ is selected so that the dissimilarity distance (Kullback-Leibler divergence) between the true posterior distribution $p(W|data)$ and the variational one is minimized. \n",
    "\n",
    "### KL divergence\n",
    "\n",
    "The KL divergence is defined as: \n",
    "\n",
    "$$KL(q||p)=\\sum_{w}q_{\\theta}(W)log\\frac{q_{\\theta}(W)}{p(W|data)} = E_{q}[log\\frac{q_{\\theta}(W)}{p(W|data)}]$$\n",
    "\n",
    "\n",
    "To be familiar with how the KL divergence is computed you can follow this link:\n",
    "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence where the similarity between a binomial distribution and discrete uniform distribution is explained using a numeric example. \n",
    "\n",
    "#### Proof of nonnegativity of KL  divergence\n",
    "\n",
    "The logarithm function that exists in the KL formula is concave and it has certainly something to do with Jensen's inequality as follows: \n",
    "\n",
    "###### Jensen's inequality\n",
    "\n",
    "For any concave function $f(x)$, \n",
    "\n",
    "$$E[f(x)] \\leq f(E[x]) $$\n",
    "\n",
    "In our case,\n",
    "\n",
    "$ f(x)= log(x)  \\implies E[log(x)] \\leq log(E[x]) $\n",
    "\n",
    "\n",
    "\n",
    "<b>Let us apply Jensen's inequality to the KL divergence. </b>\n",
    "\n",
    "$$ KL(q||p)= E_{q}[log\\frac{q_{\\theta}(W)}{p(W|data)}] $$\n",
    "\n",
    "$$ -KL(q||p)= -E_{q}[log\\frac{q_{\\theta}(W)}{p(W|data)}] = E_{q}[log\\frac{p(W|data} {q_{\\theta}(W)}] \\leq log ( E_{q}[\\frac{p(W|data)}{q_{\\theta}(W)}])$$\n",
    "\n",
    "\n",
    "$$ -KL(q||p)\\leq log(\\sum_{w} q_{\\theta}(W) [\\frac{p(W|data)}{q_{\\theta}(W)}]) $$\n",
    "\n",
    "$$ -KL(q||p)\\leq log(\\sum_{w} p(W|data)) $$\n",
    "\n",
    "$$ -KL(q||p)\\leq log(1) $$\n",
    "\n",
    "$$ -KL(q||p)\\leq 0 $$\n",
    "$$ KL(q||p)\\geq 0 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Back to the KL formula \n",
    "\n",
    "The bad news about the KL divergence is that it is a function of the true posterior distribution which is unknow. \n",
    "\n",
    "Let us rewrite its formula using Bayes Rule. \n",
    "\n",
    "$$ p(W|data)=\\frac{p(W,data)}{p(data)}$$\n",
    "\n",
    "\n",
    "$p(W,data)$: is the joint likelihood function.\n",
    "\n",
    "$p(data)$: is the marginal likelihood which can be computed by integrating the joint likelihood over W (it is independent of $w$. it is also called \"normalizing constant\" that appears in the denominator of the Bayes rule used to compute the posterior distribution. \n",
    "\n",
    "\n",
    "$$KL(q||p)=\\sum_{w}q_{\\theta}(W)log\\frac{q_{\\theta}(W)}{p(W,data)} p(data)$$ \n",
    "\n",
    "$$ =\\sum_{w}q_{\\theta}(W)log\\frac{q_{\\theta}(W)}{p(W,data)} + \\sum_{w}q_{\\theta}(W)log p(data) $$\n",
    "\n",
    "\n",
    "Let us look at the second term of the left side of the above equation\n",
    "\n",
    "$\\sum_{w}q_{\\theta}(W)log p(data) =log p(data)  \\sum_{w}q_{\\theta}(W) = logp(data).1= logp(data)$\n",
    "\n",
    "\"The sum of p(x) over all possible values of x is 1\"\n",
    "\n",
    "\n",
    "$$ KL(q||p)=\\sum_{w}q_{\\theta}(W)log\\frac{q_{\\theta}(W)}{p(W,data)} + log p(data) . $$\n",
    "\n",
    "$$ KL(q||p) - \\sum_{w}q_{\\theta}(W)log\\frac{q_{\\theta}(W)}{p(W,data)} = log p(data) ...... (*) $$\n",
    "\n",
    "\n",
    " ### $ log p(data) $ is constant and less than or equal to zero. <b> why? </b>\n",
    "\n",
    "It is constant as it results from integrating the joint likelihood over $w$. \n",
    "\n",
    "$ log p(data)\\leq 0 $ because p(data) is a probability density function with values ranging between 0 and 1 and the log of these values is always equal to or less than 0 as shown in the figure below.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Logarithm_plots.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$\\sum_{w}q_{\\theta}(W)log\\frac{q_{\\theta}(W)}{p(W,data)} = KL(q_{\\theta}||p(w,data)) \\geq 0 $  is the KL divergnece between the variational distribution and the joint likelihood which is known after a proper prior is assumed. \n",
    "\n",
    "$$ KL(q||p) - KL(q_{\\theta}||p(w,data))  = log p(data) ...... (*) $$\n",
    "\n",
    "If we write (*) in terms of the signs of the terms, we obtain\n",
    "\n",
    "nonnegative=nonnegative + nonpositive (constant)\n",
    "\n",
    "Let us have a numeric example: \n",
    "\n",
    "$KL(q||p)=3$\n",
    "\n",
    "$ -KL(q_{\\theta}||p(w,data))=-7$\n",
    "\n",
    "$ logp(data)=3 - 7 = -4 (const)$\n",
    "\n",
    "our goal is to minimize $KL(q||p)$ \n",
    "\n",
    "3 - 7 = -4 \n",
    "\n",
    "2 - 6= -4\n",
    "\n",
    "1- 5= -4 \n",
    "\n",
    "0 - 4=-4\n",
    "\n",
    "We can see that $ log p(data) \\text{ is always greater than} -KL(q_{\\theta}||p(w,data)) $\n",
    "\n",
    "- $ELBO=-KL(q_{\\theta}||p(w,data))$ is the Evidence Lower Bound (ELBO) which is the minimum value that the log    marginal likelihoos can take. That is why it is named ELBO. \n",
    "\n",
    "- Maximizing ELBO is equivalent to minimizing $KL(q||p)$  <b> COOL! </b>. That  means that the objective function (ELBO) becomes known. For optimisation purposes you can either maximize ELBO or minimize $-ELBO=KL(q_{\\theta}||p(w,data))$\n",
    "\n",
    "To summerize, in order to find an analytical approximate of a posterior distribution we need to tune the parameters of a known variational distribution ($q_{\\theta}$) so as to minimize $KL(q_{\\theta}||p(w,data))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation \n",
    "\n",
    "$$\\text{Objective function} =KL(q_{\\theta}||p(w,data))$$\n",
    "\n",
    "$$ =E_{q}[logq_{\\theta}(w)]- E_{q}[p(W,data)]$$\n",
    "\n",
    "From the above equation [1],\n",
    "\n",
    "- Term 1: $E_{q}[logq_{\\theta}(w)]=-H[q(w)]$\n",
    "\n",
    "\n",
    "H is the entropy of the variational distribution. During oprimisation process, the entropy of  $q(w)$ tends to increase. This is equivalent to maximising uncertainly by widely spreading the mass of  $q(w)$. \n",
    "\n",
    "\n",
    "- Term 2: $E_{q}[p(W,data)]$ rewards the variational distribution that places high mass where the model puts high probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic optimisation\n",
    "\n",
    "Stochastic optimisation is used to minimize the objective function $L$. The gradient of the objective function is approximated using Monte Carlo Integration using samples drawn from the variational distribution $q(w)$. The gradient here means the partial derivative of the objective function with respect to the parameters of the variational distribution ($\\theta$) i.e.  $ \\frac{\\partial L}{\\partial \\theta}$ \n",
    "\n",
    "\n",
    "The objective function is given as an expectation with respet to the variational distribution so we should expect its gradient to be given as an expectation as well due to the fact that derivatives and integrals can be exchanged. Hence, the gradient can be approximated using Monte Carlo integration. \n",
    "\n",
    "Both Paisley et al. [1] and Ranganath et al. [2] assume that the joint likelihood is independent of the parameters of the variational distribution $theta$ i.e. $$ \\frac{\\partial P(w, data)}{\\partial \\theta}=0$$ \n",
    "\n",
    "\n",
    "This assumption makes the estimate of the gradient mainly relyies on the gradient of the log of the variational distribution which is called \"score function\" [2]. As a consequence, this step can be derived once and applied to other models. For that reason, it is called \"black box variational Inference\". \n",
    "\n",
    "### Comments\n",
    "\n",
    "Using reparametrisation trick, the model's parameters $w$ can be related to the parameters of the variational distribution using a deterministic function. For instance $ w= \\mu + \\sigma \\epsilon$ where $\\epsilon \\text{~} N(0,1) $\n",
    "\n",
    "in this case using chain rule \n",
    "$$\\frac{\\partial p(w,data)}{\\partial \\theta}= \\frac{\\partial p(w,data)}{\\partial w} \\frac{\\partial w}{\\partial \\theta}$$\n",
    "\n",
    "This can be computed using a stochastic optimisation method such as SGD [3],[4],[5]. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayes with minibatch optimisation\n",
    "\n",
    "In most applications, we are given a dataset $D={(x_i, y_i)}_{i=1}^{i=N}$ where x and y represents input and target respectively. We build a model which is paramteric in our case (neural network) with a set of real-valued parameters  $W={w_i}_{i=1}^{W}$. \n",
    "\n",
    "Graves [5] used the concept of minimum length decription to describe the objective function (L) as sum of complexity loss and error loss.\n",
    "\n",
    "For one input example $(x_i, y_i)$,\n",
    "\n",
    "\n",
    "$$ L(w,\\alpha, \\theta, x_i, y_i) = E_{q}[logq_{\\theta}(w)]- E_{q}[p(W,(x_i, y_i)] $$\n",
    "\n",
    "$$  = E_{q}[logq_{\\theta}(w)]- E_{q}[p_{\\alpha}(W)] - E_{q}[P(y_i|x_i,W)] $$\n",
    "\n",
    "$$  = D_{KL}(q_{\\theta}(w)||p_{\\alpha}(w)) - E_{q}[P(y_i|x_i,W)] $$\n",
    "\n",
    "\n",
    "The error loss decreases as the prediction accuracy of the model (say neural network) increases. This error can be defined as the negative log probability of the data given the parameters (-log likelihood). It can be also called cross entropy. \n",
    "\n",
    "<center> Cross entropy= $ - \\sum_{i=1}^{N}log p(y_i|x_i, w)$ </center>\n",
    "\n",
    "\n",
    "The complexity loss can be interpreted as a reqularizer that encurages the variatioal distribution to be close to the prior distribution [3].\n",
    "\n",
    "$$  \\text{Complexity loss}= D_{KL}(q_{\\theta}(w)||p_{\\alpha}(w)) $$\n",
    "\n",
    "The complexity loss can be analytically computed as a closed form [3],[5] or approximated using Monte Carlo Integration which is efficient in practice according to [4]. \n",
    "\n",
    "###  For one input example $(x_i, y_i)  \\text{ and L samples of } W $:\n",
    "\n",
    "$$ L(w,\\alpha, \\theta, x_i, y_i)=\\frac{1}{L} \\sum_{n=1}^{L}logq_{\\theta}(w^{(n)})-logp_{\\alpha}(w^{(n)})-logp(y_i|x_i,w^{(n)})$$\n",
    "\n",
    "L is the number of samples drawn from $q_\\theta(w)$\n",
    "\n",
    "###  For a mini batch of M input examples $[(x_i, y_i)]_{i=1}^{M} \\text{ and one sample of } W $ [4], [5]:\n",
    "\n",
    "$$ L_{n}(w,\\alpha, \\theta,[(x_i, y_i)]_{i=1}^{M})=\\frac{1}{B} logq_{\\theta}(w^{(n)})-logp_{\\alpha}(w^{(n)})- \\sum_{i=1}^{i=M}logp(y_i|x_i,w^{(n)})$$\n",
    "\n",
    "If the data set is divided into equally-sized batches $ \\implies B=\\frac{N}{M} $ \n",
    "\n",
    "$$ L(w,\\alpha, \\theta,[(x_i, y_i)]_{i=1}^{M})=\\frac{1}{L} \\sum_{n=1}^{L}  L_{n}(w,\\alpha, \\theta,[(x_i, y_i)]_{i=1}^{M})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataet is large enough, setting L to 1 can result in sufficient performance. L might be increased to obtain higher accuracy but at the expense of long training time for the neural network. \n",
    "\n",
    "### The cloed form of the complexity loss\n",
    "\n",
    "If the prior is Gaussian with the parameters of $\\mu_p \\text{ and } \\sigma_p $, Hinton [6] and Graces [5] analytically derived a closed form of the complexity loss. \n",
    "\n",
    "\n",
    "$$ D_{KL}(q_{\\theta}(w)||p_{\\alpha}(w)) = log \\frac{\\sigma_p}{\\sigma_q} + \\frac{1}{2\\sigma_{p}^{2}}[\\sigma_q^2 - \\sigma_p^2 + (\\mu_p - \\mu_q)^2]$$\n",
    "\n",
    "\n",
    "The only term of the objective function that depends on the parameters of the prior is the complexity loss. Graves [5] computes the optimal values of the prior parameters that correspond to the minimum of the derivatives of this term with respect to $ \\mu_p \\text{ and } \\sigma_p $ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "\n",
    "1- Ranganath, Rajesh, Sean Gerrish, and David Blei. \"Black box variational inference.\" Artificial Intelligence and Statistics. 2014.\n",
    "\n",
    "2- Paisley, John, David Blei, and Michael Jordan. \"Variational Bayesian inference with stochastic search.\" arXiv preprint arXiv:1206.6430 (2012).\n",
    "\n",
    "3- Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013).\n",
    "\n",
    "4- Blundell, Charles, et al. \"Weight uncertainty in neural networks.\" arXiv preprint arXiv:1505.05424 (2015).\n",
    "\n",
    "5- Graves, Alex. \"Practical variational inference for neural networks.\" Advances in neural information processing systems. 2011.\n",
    "\n",
    "6- Hinton, Geoffrey E., and Drew Van Camp. \"Keeping the neural networks simple by minimizing the description length of the weights.\" Proceedings of the sixth annual conference on Computational learning theory. ACM, 1993."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
