{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blundell et al. [1] claimed that a Bayesian neural network can acheive better performance by using a scale mixture of diagonal Gaussian distributions as a prior instead of using a single Gaussian prior, see the figure below. They used MNIST data to feed a two-layered neural network to perform a classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/test_error\" height=\"420\" width=\"420\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using the analytical form, Blundell et al. approximated the expectation of the log of the prior and variational distribution with respect to the variational distrubtion using Monte Carlo integration which will be used in this work. \n",
    "\n",
    "Having seen the table of results in the figure below, a question  may arise here which is: have the authors selected the proper hyperparameters for the prior distribution before making any comparison with other works? \n",
    "\n",
    "In this notebook, we invistigate the role of the regularizer namely, complexity loss which is the sum of the log of the variational distribution and the negative of the log of the prior distribution\n",
    "\n",
    "$$ L=L1 ( \\text { Complexity loss })  + L2 ( \\text {Error loss} )$$\n",
    "\n",
    "\n",
    "Answers will be given to the follwoing two questions: \n",
    "\n",
    "1- What role does the standard deviation of the prior play?\n",
    "\n",
    "2- What role does the standard deviation of the variational distribution play?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Input data: 50 k training images containing hand-written characters.  \n",
    "Model: Two-layered neural network each hidden layer consists of 400 neurons\n",
    "Output: The output layer consists of 10 neurons which represent the target classes ranging between 0 and 9. \n",
    "\n",
    "$$ L(w,\\alpha, \\theta, x_i, y_i)=\\frac{1}{L} \\sum_{n=1}^{L}logq_{\\theta}(w^{(n)})-logp_{\\alpha}(w^{(n)})-logp(y_i|x_i,w^{(n)})$$ \n",
    "\n",
    "\n",
    "\n",
    "\"L\" represents the number of samples drawn from the variational distributions placed over the model's parameters while \"i\" indicates the index of the training example. \n",
    "\n",
    "\n",
    "The parameters of the prior distribution\n",
    "$$\\alpha = (\\mu_p=0 , \\sigma_p)$$  \n",
    "\n",
    "The parameters of the variational distribution\n",
    "$$\\theta =(\\mu , \\sigma)$$\n",
    "\n",
    "## The role of $logp(w)$\n",
    "\n",
    "\n",
    "The log prior is computed as: \n",
    "$ \\text {prior_ loss} = -\\sum_{n=1}^{L} log p(w^{(n)}) $\n",
    "\n",
    "#### Question: \n",
    "\n",
    "Does the prior loss have a negative contribution to the objective function for all the model's parameters? \n",
    "\n",
    "#### Answer: \n",
    "\n",
    "In the case of a Gaussian prior, the magnitude and sign of the contribution made by the log prior loss is mainly dependent on the parameters of the prior distribution in addition to the dynamic range of the models's parameters (Ws) which are sampled from the varioational distribution as shown in the following figures. For a standard Normal prior N(0,1), the contribution of the  prior loss is always positive with very small magnitude. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/-logp(w).png\" >\n",
    "\n",
    "- We can see from the figure above that -logp(w), except for N(0.,1.), takes positive and negative values. Why?\n",
    "\n",
    "\n",
    "$$ \\text{Prior loss}=-logp(w; 0., \\sigma_p) = log \\sqrt{2 \\pi} + log \\sigma_p + \\frac{w^2}{2\\sigma_p^2}$$\n",
    "\n",
    "\n",
    "if $\\sigma_p \\geq  1. \\implies log \\sigma_p>0 $  and the prior loss is always positive. \n",
    "\n",
    "if $\\sigma_p < 1. \\implies log \\sigma_p<0 $ and the prior loss has negative and positive regions, due to the effect of the other terms\n",
    "\n",
    "#### The derivative of prior loss with respect to W: \n",
    "\n",
    "$$ \\frac{\\partial \\text{Prior loss}}{\\partial w}=\\frac{w}{\\sigma_p^2}$$\n",
    "\n",
    "- The above equation shows that the gradient of the prior loss with respect to W has the same sign of w which means that updating Ws ($w=w-\\lambda [  \\frac{\\partial L1}{\\partial w} + \\frac{\\partial L2}{\\partial w}] )$ pushes them towards zero. Also, large weights are more penalized due to their large magnitude. Not only this, if the prior's standarded deviation  is less than one, it applies extra penalty to weights. \n",
    "\n",
    "\n",
    "- The smoothness of the prior which is a function of $\\sigma_p$ mainly affects $\\mu$. The smaller values of $\\sigma$, the sharper the prior and the more $\\mu$ is concentrated around zero (please see the section \"Histogram for $\\mu$ and $w$ \" below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function (L=L1+L2) for different priors \n",
    "\n",
    "#### LU: N(0.,1.), RU: N(0.,0.367), LD: N(0., 0.135), RD: N(0., 0.05)\n",
    "\n",
    "<table><tr><td><img src='images/1.0/loss.png'></td><td><img src='images/0.367/loss.png'></td></tr><tr><td><img src='images/0.135/loss.png'></td><td><img src='images/0.05/loss.png'></td></tr></table>\n",
    "\n",
    "#### Question: \n",
    "\n",
    "Why does the history of the loss function (L) have a linear part?\n",
    "\n",
    "#### Answer: \n",
    "\n",
    "The logq(w) is a linear function of $\\rho$ with a slope of -1. please see the section \"The role of $logq(w)$\" below. This function is responsible for making the loss function behave linearly as training progresses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Prior loss =-logp(w) for different priors \n",
    "\n",
    "#### LU: N(0.,1.), RU: N(0.,0.367), LD: N(0., 0.135), RD: N(0., 0.05)\n",
    "<table><tr><td><img src='images/1.0/log_prior.png'></td><td><img src='images/0.367/log_prior.png'></td></tr><tr><td><img src='images/0.135/log_prior.png'></td><td><img src='images/0.05/log_prior.png'></td></tr></table>\n",
    "\n",
    "From the histograms of the mean $\\mu$  and standard deviation $\\sigma$ of the resulted variaytional distributions for the model's parameters after training is completed (see figures below), we can see that as the standard deviation of the prio distribution $\\sigma_p$  increases, $ \\mu$ and $\\sigma$ increases, which in turn increases the dynamic range of the weights. This makes the prior loss (-logp(w)) increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram for $\\mu$ and $w$\n",
    "#### LU: N(0.,1.), RU: N(0.,0.367), LD: N(0., 0.135), RD: N(0., 0.05)\n",
    "\n",
    "\n",
    "<table><tr><td><img src='images/1.0/hist_mus_w.png'></td><td><img src='images/0.367/hist_mus_w.png'></td></tr><tr><td><img src='images/0.135/hist_mus_w.png'></td><td><img src='images/0.05/hist_mus_w.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram for $\\sigma$ \n",
    "#### LU: N(0.,1.), RU: N(0.,0.367), LD: N(0., 0.135), RD: N(0., 0.05)\n",
    "\n",
    "\n",
    "<table><tr><td><img src='images/1.0/hist_sigma.png'></td><td><img src='images/0.367/hist_sigma.png'></td></tr><tr><td><img src='images/0.135/hist_sigma.png'></td><td><img src='images/0.05/hist_sigma.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The regularizer without logq(w) for prior N(0,1)\n",
    "#### row 0: 1- logp(w) with logq(w), 2-logp(w) without logq(w)\n",
    "#### row1: 1- Histogram for $\\mu$ and $w$ with logq(w) 2- Histogram for $\\mu$ and $w$ without logq(w)\n",
    "\n",
    "#### row2: 1- Histogram for $\\sigma$ with logq(w) 2- Histogram for $\\sigma$ without logq(w)\n",
    " \n",
    "<table><tr><td><img src='images/1.0/log_prior.png'></td><td><img src='images/1.0/log_prior_without_logq(w).png'></td></tr><tr><td><img src='images/1.0/hist_mus_w.png'></td><td><img src='images/1.0/hist_mus_w_without_logq(w).png'></td></tr><tr><td><img src='images/1.0/hist_sigma.png'></td><td><img src='images/1.0/hist_sigma_without_logq(w).png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The regularizer without logq(w) for prior N(0,0.367)\n",
    "#### row 0: 1- logp(w) with logq(w), 2-logp(w) without logq(w)\n",
    "#### row1: 1- Histogram for $\\mu$ and $w$ with logq(w) 2- Histogram for $\\mu$ and $w$ without logq(w)\n",
    "\n",
    "#### row2: 1- Histogram for $\\sigma$ with logq(w) 2- Histogram for $\\sigma$ without logq(w)\n",
    "<table><tr><td><img src='images/0.367/log_prior.png'></td><td><img src='images/0.367/log_prior_without_logq(w).png'></td></tr><tr><td><img src='images/0.367/hist_mus_w.png'></td><td><img src='images/0.367/hist_mus_w_without_logq(w).png'></td></tr><tr><td><img src='images/0.367/hist_sigma.png'></td><td><img src='images/0.367/hist_sigma_without_logq(w).png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the removal of logq(w) shifts the distribution of $\\sigma$ downwards which emphasizes the role of logq(w) in increasing $\\sigma$ as training progresses(row 3). The distribution of w shrinks (row 2). This makes both L1 and W decrease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The role of $logq(w)$\n",
    "\n",
    "Variational distribution: $q_{\\theta}(w) = N_{\\mu, \\sigma}(w)$\n",
    "\n",
    "\n",
    "$$ logq_{\\theta}(w)= logN(w; \\mu, \\sigma)$$\n",
    "\n",
    "$$ =log[ \\frac{1}{\\sqrt{2 \\pi}\\sigma} e^{\\frac{(w-\\mu)^2}{2\\sigma^2}}]$$\n",
    "\n",
    "$$= log[ \\frac{1}{\\sigma}] + log[\\frac{1}{\\sqrt{2 \\pi}}e^{\\frac{(\\frac{w-\\mu}{\\sigma})^2}{2}}]$$\n",
    "\n",
    "\n",
    "$$ w=\\mu+\\sigma\\epsilon \\text{   ;   } \\epsilon \\text{ ~ } N(0,1)$$\n",
    "\n",
    "\n",
    "$$= log[ \\frac{1}{\\sigma}] + log[\\frac{1}{\\sqrt{2 \\pi}}e^{\\frac{\\epsilon^2}{2}}]$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ logN(w; \\mu, \\sigma)= log \\frac{1}{\\sigma}+ log N(\\epsilon, 0,1)   ....(*)$$\n",
    "\n",
    "$$ logN(w; \\mu, \\sigma)= -log\\sigma+ log N(\\epsilon, 0,1)   $$\n",
    "\n",
    "$$ \\rho=log\\sigma \\implies \\sigma=e^{\\rho} $$ \n",
    "\n",
    "$$ log q_{\\theta}(w)= logN(w; \\mu, \\sigma)= -\\rho+ log N(\\epsilon, 0,1)   ....(**)$$\n",
    "\n",
    "\n",
    "- Note that the variational loss $ logq_{\\theta}(w)$ decreases as $\\rho$ increases. This is equivalent to maximising uncertainly by widely spreading the mass of  $q(w)$. See the interpretation section in Variational Inference Part I\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.6270049], dtype=float32), array([-1.6270049], dtype=float32)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def log_gauss(x,mu=0.,sigma=0.367): \n",
    "    dist=tf.distributions.Normal(loc=mu, scale=sigma)\n",
    "    return dist.log_prob(x)\n",
    "\n",
    "\n",
    "dist=tf.distributions.Normal(loc=0., scale=1.)\n",
    "ep=dist.sample(1)\n",
    "w=2.0+2.*ep\n",
    "v1=log_gauss(w,mu=2.0,sigma=2.)\n",
    "v2=log_gauss(ep,mu=0.,sigma=1.)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run([v1, v2-np.log(2.)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $logq(w)$ is independent of $\\mu$. It consists of two terms: $log \\frac{1}{\\sigma}$  and $log N(\\epsilon, 0,1)$. The second term is independent of the parameters of $\\mu$ and $\\sigma$ while the first term decreases with increasing $\\sigma$ as shown in the figure below. This means that $\\rho$ should be increased as to  optimise the objective function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bashar/anaconda3/envs/conda_env/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5191ab3128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigma=np.linspace(0.,3., 1000)\n",
    "y=np.log(1/sigma)\n",
    "\n",
    "plt.plot(sigma,y)\n",
    "plt.xlabel('sigma')\n",
    "plt.ylabel('log(1/sigma)')\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 99.7% of the samples $\\epsilon$ drawn from $N(0,1)$ lie in the range [-3, 3]. Let us plot the function logN(0,1) for that range. $logN(0,1)$  is always negative. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logN.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $log\\sigma=-3$, using the standard Normal table we find that the percent of samples which are greater than 2.04 and less than -2.04 is 4.13%. The log of the normal distribution for these samples is less than -3.. As a conswequence 4.13% of the model parameters have negative contribution to the logq(w) loss.\n",
    "\n",
    "\n",
    "$ logq(w)=N*3+ \\sum_{i=1}^{N}logN(\\epsilon_i;0,1) \\geq 0$ N is the number of model's parameters. \n",
    "\n",
    "The table below shows how the percentage of the model's parameters, having a negative contribution to $logq(w;\\mu,\\sigma)$, changes with $log\\sigma$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/table.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logq(w)\n",
    "#### LU: N(0.,1.), RU: N(0.,0.367), LD: N(0., 0.135), RD: N(0., 0.05)\n",
    "\n",
    "<table><tr><td><img src='images/1.0/log_variational.png'></td><td><img src='images/0.367/log_variational.png'></td></tr><tr><td><img src='images/0.135/log_variational.png'></td><td><img src='images/0.05/log_variational.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How does  $\\rho$ affect $logq(w) $ and $ logp(w) $ ?\n",
    "\n",
    "This mainly depends on how significant $\\rho$ in eq(**) changes over iterations. Let us assume that the initial values of $\\rho$ are drawn fron N(-3, 0.1). We run two scenarios: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- when $\\rho$ is fixed during training\n",
    "\n",
    " Left: -logp(w), Right: logq(w)\n",
    "\n",
    "<table><tr><td><img src='images/prior_loss_untrainable_N(0,1).png'></td><td><img src='images/var_loss_untrainable_N(0,1).png'></td></tr></table>\n",
    "\n",
    "$ logq(w) $  has a Gaussian distribution due to the fact that it consists of a constant and Gaussian distribution.\n",
    "\n",
    "#### 2- when $\\rho$ is trainable \n",
    "\n",
    " Left: -logp(w), Right: logq(w)\n",
    "\n",
    "<table><tr><td><img src='images/prior_loss_trainable_N(0,1).png'></td><td><img src='images/var_loss_trainable_N(0,1).png'></td></tr></table>\n",
    "\n",
    "$logp(w)$ changes withing a small range "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Test Accuracy \n",
    "It is computed for two different priors: N(0,1) and N(0,0.367). Three scenarios are considered here:\n",
    "\n",
    "1- The full regularizer (L1) exists\n",
    "\n",
    "2- The logq(w) is removed. \n",
    "\n",
    "3- The full regularizer (L1) exists and $\\rho$ is not trainable (fixed) during training. \n",
    "\n",
    "<table>\n",
    "<tr><td>Prior</td><td> Test Accuracy</td><td> Test Accuracy without logq(w)</td> <td> with fixed $\\rho$ </td></tr>\n",
    "<tr><td>N(0,1)</td><td> 98.50</td><td> 98.54</td><td> 98.46</td></tr>\n",
    "<tr><td>N(0,0.367)</td><td> 98.56</td><td> 98.48</td><td> 98.42</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this file we have investigated the role of the regularizer including -logp(w) and logq(w) in addition to the role of $\\sigma$ of q(w) in two cases: trainable and fixed. \n",
    "\n",
    "Results show that\n",
    "\n",
    "-  the variational loss $ logq_{\\theta}(w)$ decreases as $\\sigma$ increases which leads to wide spread of the mass of $q(w)$ \n",
    "\n",
    "- With decreasing standard deviation, the prior pushes the mean of the variational distribution to concentrate around zero.\n",
    "\n",
    "- using a single gaussian prior the test accuracy for the MNIST dataset is much higher than 98.18% claimed by [1]. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this file we have investigated the role of the regularizer including -logp(w) and logq(w) in addition to the role of $\\\\sigma$ of q(w) in two cases: trainable and fixed. \n",
    "Results show that:\n",
    "-   the variational loss $ logq_{\\theta}(w)$ is independent of the mean of the variational distribution but decreases as $\\\\sigma$ increases (see how variational loss linearly decreses  with $\\sigma$), which broaden the variational distribution $q(w)$ and as a result increases parametes' uncertainity.\n",
    "   \n",
    "\n",
    "- With decreasing prior standard deviation, optimising the prior loss  pushes the mean of the variational distributions to concentrate around zero by penalyzing large weights (please refer to the derivative of the prior loss with repsect to W. Also, The prior loss is impacted by the variationl loss. Larger wights drawn from the variational distributions contributes more to prior loss.\n",
    "\n",
    "\n",
    "-  using a single gaussian prior the test accuracy for the MNIST dataset is  higher than 98.18% claimed by [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1 - Blundell, Charles, et al. \"Weight uncertainty in neural networks.\" arXiv preprint arXiv:1505.05424 (2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
